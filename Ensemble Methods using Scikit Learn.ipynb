{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "080d30cd",
   "metadata": {},
   "source": [
    "### Ensemble Methods using Scikit Learn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b49f757b",
   "metadata": {},
   "source": [
    "Ensemble methods are a type of machine learning technique that combine the predictions of multiple models to improve the overall performance of the model. The idea behind ensemble methods is that by combining multiple models, the ensemble will be more robust and less susceptible to overfitting than any individual model.\n",
    "\n",
    "There are several types of ensemble methods, let's discuss:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a33c185",
   "metadata": {},
   "source": [
    "### Bagging (Bootstrap Aggregating)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20b2de39",
   "metadata": {},
   "source": [
    "Bagging, also known as Bootstrap Aggregating, is an ensemble method that combines the predictions of multiple models trained on different subsets of the data. The idea behind bagging is that by training multiple models on different subsets of the data, the ensemble will be more robust and less susceptible to overfitting than any individual model.\n",
    "\n",
    "The basic procedure for bagging is as follows:\n",
    "\n",
    "- A dataset is randomly sampled with replacement to create multiple new datasets, called bootstrap samples.\n",
    "- A model is trained on each bootstrap sample.\n",
    "- The predictions of the individual models are combined to make a final prediction.\n",
    "\n",
    "In classification problems, the final prediction is typically made by majority voting. In regression problems, the final prediction is typically made by averaging the predictions of the individual models.\n",
    "\n",
    "Bagging can be applied to any type of model, and it is particularly useful for decision tree based models which are known to have high variance. Bagging can also be extended to random subspace method which is used for high-dimensional datasets.\n",
    "\n",
    "Bagging is also a simple but powerful ensemble method that can be used to improve the performance of a model and reduce the variance of the predictions. However, it's important to keep in mind that bagging can also be computationally expensive and requires a lot of data.\n",
    "\n",
    "Bagging can be implemented in scikit-learn using the BaggingClassifier or BaggingRegressor classes, depending on whether the task is classification or regression.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a32fade7",
   "metadata": {},
   "source": [
    "Here is an example of how to use the BaggingClassifier in scikit-learn:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "affb7c93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load the iris dataset\n",
    "iris = load_iris()\n",
    "X, y = iris.data, iris.target\n",
    "\n",
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create a decision tree classifier\n",
    "base_estimator = DecisionTreeClassifier()\n",
    "\n",
    "# Create a bagging classifier ( explanation=n_estimators parameter sets the number of base estimators in the ensemble, max_samples sets the proportion of samples to be used for fitting the base estimator, and max_features sets the proportion of features to be used for fitting the base estimator. )\n",
    "bagging = BaggingClassifier(base_estimator=base_estimator, n_estimators=10, max_samples=0.8, max_features=0.8)\n",
    "\n",
    "# Fit the bagging classifier on the training data\n",
    "bagging.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test data\n",
    "y_pred = bagging.predict(X_test)\n",
    "\n",
    "# Evaluate the performance of the ensemble\n",
    "from sklearn.metrics import accuracy_score\n",
    "print(accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c34fe69",
   "metadata": {},
   "source": [
    "Here is an example of how to use the BaggingRegressor in scikit-learn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5f60db09",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.271417647058824\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:87: FutureWarning: Function load_boston is deprecated; `load_boston` is deprecated in 1.0 and will be removed in 1.2.\n",
      "\n",
      "    The Boston housing prices dataset has an ethical problem. You can refer to\n",
      "    the documentation of this function for further details.\n",
      "\n",
      "    The scikit-learn maintainers therefore strongly discourage the use of this\n",
      "    dataset unless the purpose of the code is to study and educate about\n",
      "    ethical issues in data science and machine learning.\n",
      "\n",
      "    In this special case, you can fetch the dataset from the original\n",
      "    source::\n",
      "\n",
      "        import pandas as pd\n",
      "        import numpy as np\n",
      "\n",
      "\n",
      "        data_url = \"http://lib.stat.cmu.edu/datasets/boston\"\n",
      "        raw_df = pd.read_csv(data_url, sep=\"\\s+\", skiprows=22, header=None)\n",
      "        data = np.hstack([raw_df.values[::2, :], raw_df.values[1::2, :2]])\n",
      "        target = raw_df.values[1::2, 2]\n",
      "\n",
      "    Alternative datasets include the California housing dataset (i.e.\n",
      "    :func:`~sklearn.datasets.fetch_california_housing`) and the Ames housing\n",
      "    dataset. You can load the datasets as follows::\n",
      "\n",
      "        from sklearn.datasets import fetch_california_housing\n",
      "        housing = fetch_california_housing()\n",
      "\n",
      "    for the California housing dataset and::\n",
      "\n",
      "        from sklearn.datasets import fetch_openml\n",
      "        housing = fetch_openml(name=\"house_prices\", as_frame=True)\n",
      "\n",
      "    for the Ames housing dataset.\n",
      "    \n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import BaggingRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.datasets import load_boston\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load the boston dataset\n",
    "boston = load_boston()\n",
    "X, y = boston.data, boston.target\n",
    "\n",
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create a decision tree regressor\n",
    "base_estimator = DecisionTreeRegressor()\n",
    "\n",
    "# Create a bagging regressor ( explanation=n_estimators parameter sets the number of base estimators in the ensemble, max_samples sets the proportion of samples to be used for fitting the base estimator, and max_features sets the proportion of features to be used for fitting the base estimator. )\n",
    "bagging = BaggingRegressor(base_estimator=base_estimator, n_estimators=10, max_samples=0.8, max_features=0.8)\n",
    "\n",
    "# Fit the bagging regressor on the training data\n",
    "bagging.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test data\n",
    "y_pred = bagging.predict(X_test)\n",
    "\n",
    "# Evaluate the performance of the ensemble\n",
    "from sklearn.metrics import mean_squared_error\n",
    "print(mean_squared_error(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6ffe22a",
   "metadata": {},
   "source": [
    "### Boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7888412",
   "metadata": {},
   "source": [
    "Boosting is an ensemble method that improves the performance of a weak model by iteratively fitting the model on different subsets of the data, and then adjusting the weights of the observations to focus on the observations that are misclassified. The idea behind boosting is to combine the predictions of multiple weak models to create a strong model that is more accurate than any individual weak model.\n",
    "\n",
    "Boosting algorithms can be broadly classified into two categories:\n",
    "\n",
    "__Adaptive Boosting (AdaBoost):__ AdaBoost is the first and most popular boosting algorithm that was proposed by Freund and Schapire in 1996. AdaBoost works by iteratively fitting a weak model, such as a decision tree with a small number of splits, on the data and adjusting the weights of the observations to focus on the observations that are misclassified by the previous weak model.\n",
    "\n",
    "__Gradient Boosting:__ Gradient Boosting is a more recent boosting algorithm that was proposed by Friedman in 1999. Instead of adjusting the weights of the observations, gradient boosting adjusts the predictions of the weak model by fitting a gradient descent algorithm to minimize the loss function. The most popular gradient boosting algorithm is XGBoost and LightGBM.\n",
    "\n",
    "Boosting algorithms are considered powerful algorithms that can be used to improve the performance of a model and reduce the bias of the predictions. However, it's important to keep in mind that boosting algorithms can also be computationally expensive and require a lot of data. Additionally, boosting algorithms can also be sensitive to noise and overfitting if the weak model is too complex or if the number of iterations is too large."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c4bf0e1",
   "metadata": {},
   "source": [
    "here is an example of how to use the AdaBoostClassifier in scikit-learn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8f14d323",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load the iris dataset\n",
    "iris = load_iris()\n",
    "X, y = iris.data, iris.target\n",
    "\n",
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create a decision tree classifier\n",
    "base_estimator = DecisionTreeClassifier()\n",
    "\n",
    "# Create an AdaBoost classifier(The n_estimators parameter sets the number of base estimators in the ensemble, learning_rate controls the contribution of each classifier.)\n",
    "ada_boost = AdaBoostClassifier(base_estimator=base_estimator, n_estimators=50, learning_rate=1)\n",
    "\n",
    "# Fit the AdaBoost classifier on the training data\n",
    "ada_boost.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test data\n",
    "y_pred = ada_boost.predict(X_test)\n",
    "\n",
    "# Evaluate the performance of the ensemble\n",
    "from sklearn.metrics import accuracy_score\n",
    "print(accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87e840c9",
   "metadata": {},
   "source": [
    "here is an example of how to use the AdaBoostRegressor in scikit-learn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2d05bc2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11.084999999999996\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:87: FutureWarning: Function load_boston is deprecated; `load_boston` is deprecated in 1.0 and will be removed in 1.2.\n",
      "\n",
      "    The Boston housing prices dataset has an ethical problem. You can refer to\n",
      "    the documentation of this function for further details.\n",
      "\n",
      "    The scikit-learn maintainers therefore strongly discourage the use of this\n",
      "    dataset unless the purpose of the code is to study and educate about\n",
      "    ethical issues in data science and machine learning.\n",
      "\n",
      "    In this special case, you can fetch the dataset from the original\n",
      "    source::\n",
      "\n",
      "        import pandas as pd\n",
      "        import numpy as np\n",
      "\n",
      "\n",
      "        data_url = \"http://lib.stat.cmu.edu/datasets/boston\"\n",
      "        raw_df = pd.read_csv(data_url, sep=\"\\s+\", skiprows=22, header=None)\n",
      "        data = np.hstack([raw_df.values[::2, :], raw_df.values[1::2, :2]])\n",
      "        target = raw_df.values[1::2, 2]\n",
      "\n",
      "    Alternative datasets include the California housing dataset (i.e.\n",
      "    :func:`~sklearn.datasets.fetch_california_housing`) and the Ames housing\n",
      "    dataset. You can load the datasets as follows::\n",
      "\n",
      "        from sklearn.datasets import fetch_california_housing\n",
      "        housing = fetch_california_housing()\n",
      "\n",
      "    for the California housing dataset and::\n",
      "\n",
      "        from sklearn.datasets import fetch_openml\n",
      "        housing = fetch_openml(name=\"house_prices\", as_frame=True)\n",
      "\n",
      "    for the Ames housing dataset.\n",
      "    \n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.datasets import load_boston\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load the boston dataset\n",
    "boston = load_boston()\n",
    "X, y = boston.data, boston.target\n",
    "\n",
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create a decision tree regressor\n",
    "base_estimator = DecisionTreeRegressor()\n",
    "\n",
    "# Create an AdaBoost regressor\n",
    "ada_boost = AdaBoostRegressor(base_estimator=base_estimator, n_estimators=50, learning_rate=1)\n",
    "\n",
    "# Fit the AdaBoost regressor on the training data\n",
    "ada_boost.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test data\n",
    "y_pred = ada_boost.predict(X_test)\n",
    "\n",
    "# Evaluate the performance of the ensemble\n",
    "from sklearn.metrics import mean_squared_error\n",
    "print(mean_squared_error(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b53c85b5",
   "metadata": {},
   "source": [
    "### Gradient Boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62b1bcb9",
   "metadata": {},
   "source": [
    "Gradient Boosting is a specific implementation of the boosting algorithm that uses gradient descent to minimize the loss function of the weak model. The most popular gradient boosting algorithms are XGBoost and LightGBM.\n",
    "\n",
    "Gradient Boosting works by iteratively fitting a weak model, such as a decision tree, on the residuals of the previous model. The residuals are the differences between the true values and the predictions of the previous model. The new model is then added to the ensemble and the process is repeated until a certain number of models is reached or the improvement in the loss function is below a certain threshold.\n",
    "\n",
    "The main advantage of gradient boosting is its ability to handle large datasets and high-dimensional feature spaces. Additionally, gradient boosting can also be used for both regression and classification problems, and it's relatively insensitive to the choice of the weak model.\n",
    "\n",
    "The main disadvantage of gradient boosting is its sensitivity to overfitting if the number of iterations is too high or the weak model is too complex. Additionally, gradient boosting can also be computationally expensive and require a lot of memory.\n",
    "\n",
    "You can use GradientBoostingRegressor or GradientBoostingClassifier from sklearn.ensemble to implement gradient boosting in your project.\n",
    "\n",
    "\n",
    "There are several types of gradient boosting algorithms, but the most popular ones are:\n",
    "\n",
    "__XGBoost (eXtreme Gradient Boosting):__ XGBoost is an optimized version of gradient boosting that is designed to be more efficient and faster than other gradient boosting libraries. It was developed by Tianqi Chen and was first released in 2014. XGBoost is known for its good performance on large datasets and high-dimensional feature spaces.\n",
    "\n",
    "__LightGBM:__ LightGBM is another gradient boosting library that is designed to be more efficient and faster than other gradient boosting libraries. It was developed by Microsoft and was first released in 2017. LightGBM is known for its good performance on large datasets and high-dimensional feature spaces and for its ability to handle categorical features without one-hot encoding."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "942f55aa",
   "metadata": {},
   "source": [
    "here is an example of how to use the GradientBoostingClassifier in scikit-learn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d438c197",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load the iris dataset\n",
    "iris = load_iris()\n",
    "X, y = iris.data, iris.target\n",
    "\n",
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create a Gradient Boosting classifier\n",
    "gb = GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, max_depth=1)\n",
    "\n",
    "# Fit the Gradient Boosting classifier on the training data\n",
    "gb.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test data\n",
    "y_pred = gb.predict(X_test)\n",
    "\n",
    "# Evaluate the performance of the ensemble\n",
    "from sklearn.metrics import accuracy_score\n",
    "print(accuracy_score(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4757e2b",
   "metadata": {},
   "source": [
    " here is an example of how to use the GradientBoostingRegressor in scikit-learn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8dcf137d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14.52681979717445\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:87: FutureWarning: Function load_boston is deprecated; `load_boston` is deprecated in 1.0 and will be removed in 1.2.\n",
      "\n",
      "    The Boston housing prices dataset has an ethical problem. You can refer to\n",
      "    the documentation of this function for further details.\n",
      "\n",
      "    The scikit-learn maintainers therefore strongly discourage the use of this\n",
      "    dataset unless the purpose of the code is to study and educate about\n",
      "    ethical issues in data science and machine learning.\n",
      "\n",
      "    In this special case, you can fetch the dataset from the original\n",
      "    source::\n",
      "\n",
      "        import pandas as pd\n",
      "        import numpy as np\n",
      "\n",
      "\n",
      "        data_url = \"http://lib.stat.cmu.edu/datasets/boston\"\n",
      "        raw_df = pd.read_csv(data_url, sep=\"\\s+\", skiprows=22, header=None)\n",
      "        data = np.hstack([raw_df.values[::2, :], raw_df.values[1::2, :2]])\n",
      "        target = raw_df.values[1::2, 2]\n",
      "\n",
      "    Alternative datasets include the California housing dataset (i.e.\n",
      "    :func:`~sklearn.datasets.fetch_california_housing`) and the Ames housing\n",
      "    dataset. You can load the datasets as follows::\n",
      "\n",
      "        from sklearn.datasets import fetch_california_housing\n",
      "        housing = fetch_california_housing()\n",
      "\n",
      "    for the California housing dataset and::\n",
      "\n",
      "        from sklearn.datasets import fetch_openml\n",
      "        housing = fetch_openml(name=\"house_prices\", as_frame=True)\n",
      "\n",
      "    for the Ames housing dataset.\n",
      "    \n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.datasets import load_boston\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load the boston dataset\n",
    "boston = load_boston()\n",
    "X, y = boston.data, boston.target\n",
    "\n",
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create a Gradient Boosting regressor\n",
    "gb = GradientBoostingRegressor(n_estimators=100, learning_rate=0.1, max_depth=1)\n",
    "\n",
    "# Fit the Gradient Boosting regressor on the training data\n",
    "gb.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test data\n",
    "y_pred = gb.predict(X_test)\n",
    "\n",
    "# Evaluate the performance of the ensemble\n",
    "from sklearn.metrics import mean_squared_error\n",
    "print(mean_squared_error(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02542e36",
   "metadata": {},
   "source": [
    "### XGBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dfbc408",
   "metadata": {},
   "source": [
    "XGBoost (eXtreme Gradient Boosting) is an open-source library for gradient boosting that is designed to be more efficient and faster than other gradient boosting libraries. XGBoost was developed by Tianqi Chen and was first released in 2014. It is widely used in machine learning competitions and has been a go-to tool for many data scientists.\n",
    "\n",
    "XGBoost has several advantages over other gradient boosting libraries:\n",
    "\n",
    "- It is designed to handle large datasets and high-dimensional feature spaces.\n",
    "- It uses a technique called \"out-of-core\" learning, which allows it to handle data that is too large to fit in memory.\n",
    "- It supports parallel processing, which makes it faster than other gradient boosting libraries.\n",
    "- It has a built-in regularization mechanism that helps prevent overfitting.\n",
    "- It includes several features such as early stopping, cross-validation, and automatic handling of missing values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc1d8daf",
   "metadata": {},
   "source": [
    "Here is an example of how to use the XGBClassifier in scikit-learn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "454c4c95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBClassifier\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load the iris dataset\n",
    "iris = load_iris()\n",
    "X, y = iris.data, iris.target\n",
    "\n",
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create a XGBoost classifier\n",
    "xgb_clf = XGBClassifier(n_estimators=100, learning_rate=0.1, max_depth=1)\n",
    "\n",
    "# Fit the XGBoost classifier on the training data\n",
    "xgb_clf.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test data\n",
    "y_pred = xgb_clf.predict(X_test)\n",
    "\n",
    "# Evaluate the performance of the ensemble\n",
    "from sklearn.metrics import accuracy_score\n",
    "print(accuracy_score(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "450e8632",
   "metadata": {},
   "source": [
    "Here is an example of how to use the XGBRegressor in scikit-learn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b1847a18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14.398599735606238\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:87: FutureWarning: Function load_boston is deprecated; `load_boston` is deprecated in 1.0 and will be removed in 1.2.\n",
      "\n",
      "    The Boston housing prices dataset has an ethical problem. You can refer to\n",
      "    the documentation of this function for further details.\n",
      "\n",
      "    The scikit-learn maintainers therefore strongly discourage the use of this\n",
      "    dataset unless the purpose of the code is to study and educate about\n",
      "    ethical issues in data science and machine learning.\n",
      "\n",
      "    In this special case, you can fetch the dataset from the original\n",
      "    source::\n",
      "\n",
      "        import pandas as pd\n",
      "        import numpy as np\n",
      "\n",
      "\n",
      "        data_url = \"http://lib.stat.cmu.edu/datasets/boston\"\n",
      "        raw_df = pd.read_csv(data_url, sep=\"\\s+\", skiprows=22, header=None)\n",
      "        data = np.hstack([raw_df.values[::2, :], raw_df.values[1::2, :2]])\n",
      "        target = raw_df.values[1::2, 2]\n",
      "\n",
      "    Alternative datasets include the California housing dataset (i.e.\n",
      "    :func:`~sklearn.datasets.fetch_california_housing`) and the Ames housing\n",
      "    dataset. You can load the datasets as follows::\n",
      "\n",
      "        from sklearn.datasets import fetch_california_housing\n",
      "        housing = fetch_california_housing()\n",
      "\n",
      "    for the California housing dataset and::\n",
      "\n",
      "        from sklearn.datasets import fetch_openml\n",
      "        housing = fetch_openml(name=\"house_prices\", as_frame=True)\n",
      "\n",
      "    for the Ames housing dataset.\n",
      "    \n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBRegressor\n",
    "from sklearn.datasets import load_boston\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load the boston dataset\n",
    "boston = load_boston()\n",
    "X, y = boston.data, boston.target\n",
    "\n",
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create a XGBoost regressor\n",
    "xgb_reg = XGBRegressor(n_estimators=100, learning_rate=0.1, max_depth=1)\n",
    "\n",
    "# Fit the XGBoost regressor on the training data\n",
    "xgb_reg.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test data\n",
    "y_pred = xgb_reg.predict(X_test)\n",
    "\n",
    "# Evaluate the performance of the ensemble\n",
    "from sklearn.metrics import mean_squared_error\n",
    "print(mean_squared_error(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb531176",
   "metadata": {},
   "source": [
    "### LightGBM\n",
    "\n",
    "LightGBM is another popular gradient boosting library that is designed to be faster and more efficient than other libraries like XGBoost. LightGBM uses a technique called \"gradient-based one-side sampling\" to reduce the data used in the tree-growing process, which speeds up the training time. Additionally, LightGBM uses a technique called \"leaf-wise\" tree-growing, which results in deeper trees and better performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76a219d4",
   "metadata": {},
   "source": [
    "Here is an example of how to use the LGBMClassifier in scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "98215673",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n"
     ]
    }
   ],
   "source": [
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load the iris dataset\n",
    "iris = load_iris()\n",
    "X, y = iris.data, iris.target\n",
    "\n",
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create a LightGBM classifier\n",
    "lgb_clf = LGBMClassifier(n_estimators=100, learning_rate=0.1, max_depth=1)\n",
    "\n",
    "# Fit the LightGBM classifier on the training data\n",
    "lgb_clf.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test data\n",
    "y_pred = lgb_clf.predict(X_test)\n",
    "\n",
    "# Evaluate the performance of the ensemble\n",
    "from sklearn.metrics import accuracy_score\n",
    "print(accuracy_score(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "560b9cf8",
   "metadata": {},
   "source": [
    "Here is an example of how to use the LGBMRegressor in scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a940794d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14.33363981900555\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:87: FutureWarning: Function load_boston is deprecated; `load_boston` is deprecated in 1.0 and will be removed in 1.2.\n",
      "\n",
      "    The Boston housing prices dataset has an ethical problem. You can refer to\n",
      "    the documentation of this function for further details.\n",
      "\n",
      "    The scikit-learn maintainers therefore strongly discourage the use of this\n",
      "    dataset unless the purpose of the code is to study and educate about\n",
      "    ethical issues in data science and machine learning.\n",
      "\n",
      "    In this special case, you can fetch the dataset from the original\n",
      "    source::\n",
      "\n",
      "        import pandas as pd\n",
      "        import numpy as np\n",
      "\n",
      "\n",
      "        data_url = \"http://lib.stat.cmu.edu/datasets/boston\"\n",
      "        raw_df = pd.read_csv(data_url, sep=\"\\s+\", skiprows=22, header=None)\n",
      "        data = np.hstack([raw_df.values[::2, :], raw_df.values[1::2, :2]])\n",
      "        target = raw_df.values[1::2, 2]\n",
      "\n",
      "    Alternative datasets include the California housing dataset (i.e.\n",
      "    :func:`~sklearn.datasets.fetch_california_housing`) and the Ames housing\n",
      "    dataset. You can load the datasets as follows::\n",
      "\n",
      "        from sklearn.datasets import fetch_california_housing\n",
      "        housing = fetch_california_housing()\n",
      "\n",
      "    for the California housing dataset and::\n",
      "\n",
      "        from sklearn.datasets import fetch_openml\n",
      "        housing = fetch_openml(name=\"house_prices\", as_frame=True)\n",
      "\n",
      "    for the Ames housing dataset.\n",
      "    \n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "from lightgbm import LGBMRegressor\n",
    "from sklearn.datasets import load_boston\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load the boston dataset\n",
    "boston = load_boston()\n",
    "X, y = boston.data, boston.target\n",
    "\n",
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create a LightGBM regressor\n",
    "lgb_reg = LGBMRegressor(n_estimators=100, learning_rate=0.1, max_depth=1)\n",
    "\n",
    "# Fit the LightGBM regressor on the training data\n",
    "lgb_reg.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test data\n",
    "y_pred = lgb_reg.predict(X_test)\n",
    "\n",
    "# Evaluate the performance of the ensemble\n",
    "from sklearn.metrics import mean_squared_error\n",
    "print(mean_squared_error(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3db0fea7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting lightgbm\n",
      "  Downloading lightgbm-3.3.4-py3-none-win_amd64.whl (1.0 MB)\n",
      "     ---------------------------------------- 1.0/1.0 MB 1.2 MB/s eta 0:00:00\n",
      "Requirement already satisfied: scikit-learn!=0.22.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from lightgbm) (1.0.2)\n",
      "Requirement already satisfied: wheel in c:\\users\\admin\\anaconda3\\lib\\site-packages (from lightgbm) (0.37.1)\n",
      "Requirement already satisfied: scipy in c:\\users\\admin\\anaconda3\\lib\\site-packages (from lightgbm) (1.9.1)\n",
      "Requirement already satisfied: numpy in c:\\users\\admin\\anaconda3\\lib\\site-packages (from lightgbm) (1.21.5)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from scikit-learn!=0.22.0->lightgbm) (2.2.0)\n",
      "Requirement already satisfied: joblib>=0.11 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from scikit-learn!=0.22.0->lightgbm) (1.1.0)\n",
      "Installing collected packages: lightgbm\n",
      "Successfully installed lightgbm-3.3.4\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install lightgbm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0abd1841",
   "metadata": {},
   "source": [
    "### Random Forest ensemble method\n",
    "\n",
    "Random Forest is an ensemble machine learning method that is used for both classification and regression problems. It is an extension of decision trees and is based on the idea of training multiple decision trees and combining their predictions.\n",
    "\n",
    "The basic idea behind Random Forest is to randomly select a subset of the training data, and train a decision tree on each subset. The final prediction is made by averaging the predictions of all the trees in the forest. The randomness in the selection of subsets of the data and features used to split the nodes in the decision tree, provides a degree of randomness in the predictions made by the individual trees, which helps to reduce overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52a95b2c",
   "metadata": {},
   "source": [
    "Here is an example of how to use the RandomForestClassifier in scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "45e0653a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9666666666666667\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load the iris dataset\n",
    "iris = load_iris()\n",
    "X, y = iris.data, iris.target\n",
    "\n",
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create a Random Forest classifier\n",
    "rf_clf = RandomForestClassifier(n_estimators=100, max_depth=1)\n",
    "\n",
    "# Fit the Random Forest classifier on the training data\n",
    "rf_clf.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test data\n",
    "y_pred = rf_clf.predict(X_test)\n",
    "\n",
    "# Evaluate the performance of the ensemble\n",
    "from sklearn.metrics import accuracy_score\n",
    "print(accuracy_score(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d6d6bd2",
   "metadata": {},
   "source": [
    "Here is an example of how to use the RandomForestRegressor in scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "402c9e50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34.49386518215856\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:87: FutureWarning: Function load_boston is deprecated; `load_boston` is deprecated in 1.0 and will be removed in 1.2.\n",
      "\n",
      "    The Boston housing prices dataset has an ethical problem. You can refer to\n",
      "    the documentation of this function for further details.\n",
      "\n",
      "    The scikit-learn maintainers therefore strongly discourage the use of this\n",
      "    dataset unless the purpose of the code is to study and educate about\n",
      "    ethical issues in data science and machine learning.\n",
      "\n",
      "    In this special case, you can fetch the dataset from the original\n",
      "    source::\n",
      "\n",
      "        import pandas as pd\n",
      "        import numpy as np\n",
      "\n",
      "\n",
      "        data_url = \"http://lib.stat.cmu.edu/datasets/boston\"\n",
      "        raw_df = pd.read_csv(data_url, sep=\"\\s+\", skiprows=22, header=None)\n",
      "        data = np.hstack([raw_df.values[::2, :], raw_df.values[1::2, :2]])\n",
      "        target = raw_df.values[1::2, 2]\n",
      "\n",
      "    Alternative datasets include the California housing dataset (i.e.\n",
      "    :func:`~sklearn.datasets.fetch_california_housing`) and the Ames housing\n",
      "    dataset. You can load the datasets as follows::\n",
      "\n",
      "        from sklearn.datasets import fetch_california_housing\n",
      "        housing = fetch_california_housing()\n",
      "\n",
      "    for the California housing dataset and::\n",
      "\n",
      "        from sklearn.datasets import fetch_openml\n",
      "        housing = fetch_openml(name=\"house_prices\", as_frame=True)\n",
      "\n",
      "    for the Ames housing dataset.\n",
      "    \n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.datasets import load_boston\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load the boston dataset\n",
    "boston = load_boston()\n",
    "X, y = boston.data, boston.target\n",
    "\n",
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create a Random Forest regressor\n",
    "rf_reg = RandomForestRegressor(n_estimators=100, max_depth=1)\n",
    "\n",
    "# Fit the Random Forest regressor on the training data\n",
    "rf_reg.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test data\n",
    "y_pred = rf_reg.predict(X_test)\n",
    "\n",
    "# Evaluate the performance of the ensemble\n",
    "from sklearn.metrics import mean_squared_error\n",
    "print(mean_squared_error(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9a87727",
   "metadata": {},
   "source": [
    "##### Md. Ashiqur Rahman\n",
    "##### Thank You"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
