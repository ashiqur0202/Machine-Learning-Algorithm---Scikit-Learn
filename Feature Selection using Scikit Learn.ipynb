{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "100c9a1c-1659-4ecb-8f5b-c3255ad975b1",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Feature Selection using Scikit Learn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61ffa5e6-0048-4e3a-abd4-0f3e139e2b25",
   "metadata": {},
   "source": [
    "__Feature selection__\n",
    "\n",
    "Feature selection is the process of identifying a subset of the most relevant features from a larger set of features for use in building a machine learning model. The goal of feature selection is to improve the performance of the model by reducing overfitting, increasing interpretability, and reducing the computational cost of training and deploying the model.\n",
    "\n",
    "There are several techniques that can be used for feature selection, including:\n",
    "\n",
    "__Filter methods:__ These methods are based on a statistical test to evaluate the relevance of each feature individually, and select the features with the highest scores. Examples of filter methods include chi-squared test, mutual information, and correlation coefficient.\n",
    "\n",
    "__Wrapper methods:__ These methods use a specific machine learning algorithm to evaluate the performance of subsets of features and select the subset that performs the best. Examples of wrapper methods include forward selection, backward elimination, and recursive feature elimination.\n",
    "\n",
    "__Embedded methods:__ These methods incorporate feature selection as part of the learning process, and are based on the regularization of the model parameters. Examples of embedded methods include Lasso, Ridge, and Elastic Net.\n",
    "\n",
    "__Hybrid methods:__ These methods combine two or more of the above methods to improve the performance of feature selection.\n",
    "\n",
    "The choice of feature selection method depends on the specific problem, the size and nature of the data, and the resources available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1480abbf-925e-4e42-b469-86b0a426fa17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected features: ['petal length (cm)' 'petal width (cm)']\n",
      "Test set accuracy: 1.00\n"
     ]
    }
   ],
   "source": [
    "# Here is a demo of Feature Selection\n",
    "import numpy as np\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load the iris dataset\n",
    "iris = load_iris()\n",
    "X, y = iris.data, iris.target\n",
    "\n",
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Use SelectKBest with f_classif to select the top 2 features\n",
    "selector = SelectKBest(f_classif, k=2)\n",
    "X_new = selector.fit_transform(X_train, y_train)\n",
    "\n",
    "# Print the selected features\n",
    "selected_features = np.array(iris.feature_names)[np.where(selector.get_support())]\n",
    "print(f'Selected features: {selected_features}')\n",
    "\n",
    "# Train a logistic regression model on the selected features\n",
    "clf = LogisticRegression()\n",
    "clf.fit(X_new, y_train)\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "accuracy = clf.score(selector.transform(X_test), y_test)\n",
    "print(f'Test set accuracy: {accuracy:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fcbb514-338f-45d9-9a16-82ddce5689ad",
   "metadata": {},
   "source": [
    "__Filter methods__\n",
    "\n",
    "Filter methods for feature selection are a type of feature selection technique that evaluates the relevance of each feature individually, without considering the interactions between features or the relationship between the features and the target variable. The goal is to select the features that have the highest scores based on a predefined evaluation criterion. The main advantage of filter methods is that they are computationally efficient and easy to implement. Some examples of filter methods include:\n",
    "\n",
    "__Chi-squared test:__ It is a statistical test that measures the correlation between a feature and the target variable. It is commonly used for categorical features and binary classification problems.\n",
    "\n",
    "__Mutual information:__ it measures the dependence between two variables, and is commonly used for both categorical and continuous features.\n",
    "\n",
    "__Correlation coefficient:__ It measures the linear relationship between two variables and is commonly used for continuous features.\n",
    "\n",
    "__F-score :__ It combines the precision and recall of a feature with respect to the target variable.\n",
    "\n",
    "__Information gain:__ It measures the reduction in entropy caused by a feature with respect to the target variable.\n",
    "\n",
    "__Correlation-based Feature Selection (CFS) :__ It is filter method that evaluates the correlation between feature and class.\n",
    "\n",
    "__Mutual Information-based Feature Selection (MIFS):__ It is filter method that evaluates the mutual information between feature and class.\n",
    "\n",
    "__Variance Threshold:__ It is a filter method that removes all features whose variance doesnâ€™t meet some threshold.\n",
    "\n",
    "These filter methods can be used as a pre-processing step before training a machine learning model, and can be used with any machine learning algorithm. However, filter methods do not consider the relationship between features, so they may not always identify the best set of features for a particular problem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2b7e028-851e-4b84-a8f4-0266c1efd6c3",
   "metadata": {},
   "source": [
    "__Wrapper methods__\n",
    "\n",
    "Wrapper methods for feature selection are a type of feature selection technique that use a specific machine learning algorithm to evaluate the performance of subsets of features and select the subset that performs the best. The goal is to find the subset of features that results in the best performance of the machine learning model. The main advantage of wrapper methods is that they consider the relationship between features and the target variable and therefore may identify a more informative subset of features. Some examples of wrapper methods include:\n",
    "\n",
    "__Forward Selection:__ It starts with an empty set of features and adds one feature at a time based on the performance of the model.\n",
    "\n",
    "__Backward Elimination:__ It starts with all the features and removes one feature at a time based on the performance of the model.\n",
    "\n",
    "__Recursive Feature Elimination (RFE):__ It starts with all the features and recursively eliminates features by building a model using subsets of features and ranking them by their importance.\n",
    "\n",
    "__Genetic Algorithm:__ it is based on the concept of natural selection and genetic operators such as mutation, crossover and selection, and it's applied to select a subset of features.\n",
    "\n",
    "__Particle Swarm Optimization (PSO):__ It is a heuristic optimization algorithm that simulates the social behavior of birds and bees, and it's applied to select a subset of features.\n",
    "\n",
    "Wrapper methods can be computationally expensive and require a significant amount of time to perform, especially when the number of features is high. Additionally, wrapper methods are dependent on the specific machine learning algorithm used, so the results may vary based on the choice of algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab3492ec-e2c6-487b-84c7-1d87c08fd574",
   "metadata": {},
   "source": [
    "__Embedded methods__\n",
    "\n",
    "Embedded methods for feature selection are a type of feature selection technique that incorporate feature selection as part of the learning process, and are based on the regularization of the model parameters. The goal is to find a set of features that results in the best performance of the machine learning model while also reducing the complexity of the model. The main advantage of embedded methods is that they are integrated with the learning algorithm and can therefore be optimized along with the other model parameters. Some examples of embedded methods include:\n",
    "\n",
    "__Lasso (Least Absolute Shrinkage and Selection Operator):__ It is a linear model that uses L1 regularization to shrink the absolute value of the coefficients of the features towards zero, effectively dropping some features from the model.\n",
    "\n",
    "__Ridge Regression:__ it is a linear model that uses L2 regularization to shrink the coefficients of the features towards zero, effectively dropping some features from the model.\n",
    "\n",
    "__Elastic Net:__ It is a linear model that combines L1 and L2 regularization, which can balance the trade-off between sparsity and small-sample bias.\n",
    "\n",
    "__Random Forest:__ it is a tree-based model that uses feature importance to select features.\n",
    "\n",
    "__Gradient Boosting:__ it is a tree-based model that uses feature importance to select features.\n",
    "\n",
    "__Linear Discriminant Analysis (LDA) :__ it is a linear model that uses the discriminant information between classes to select features.\n",
    "\n",
    "Embedded methods can be more computationally efficient than wrapper methods and can be used with any machine learning algorithm. However, embedded methods are dependent on the specific machine learning algorithm used, and the results may vary based on the choice of algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d307c63-5926-4c20-8bd0-446e05523212",
   "metadata": {},
   "source": [
    "__Hybrid methods__\n",
    "\n",
    "Hybrid methods for feature selection are a type of feature selection technique that combine two or more of the above methods to improve the performance of feature selection. The goal is to take advantage of the strengths of different feature selection methods and to overcome their limitations. The main advantage of hybrid methods is that they can achieve better performance than using a single method alone. Some examples of hybrid methods include:\n",
    "\n",
    "__Filter-Wrapper Method:__ It combines a filter method with a wrapper method by first using a filter method to pre-select a set of features and then using a wrapper method to fine-tune the selection.\n",
    "\n",
    "__Wrapper-Embedded Method:__ It combines a wrapper method with an embedded method by first using a wrapper method to select a subset of features and then using an embedded method to optimize the model parameters.\n",
    "\n",
    "__Filter-Embedded Method:__ It combines a filter method with an embedded method by first using a filter method to select a subset of features and then using an embedded method to optimize the model parameters.\n",
    "\n",
    "__Filter-Wrapper-Embedded Method:__ It combines filter-wrapper and wrapper-embedded method to select features.\n",
    "\n",
    "Hybrid methods can be computationally expensive and require a significant amount of time to perform, especially when the number of features is high. Additionally, hybrid methods are dependent on the specific combination of methods used, so the results may vary based on the choice of methods."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40b0e381-4922-467e-a051-ca2cf0199d61",
   "metadata": {},
   "source": [
    "##### Md. Ashiqur Rahman\n",
    "##### Thank You"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
